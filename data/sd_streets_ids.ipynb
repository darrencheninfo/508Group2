{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"sd_streets_ids.ipynb","provenance":[{"file_id":"1T5qUqZdy5W_ILvysWAoUIljuthY1k5kQ","timestamp":1654451560108}],"collapsed_sections":[],"authorship_tag":"ABX9TyO+0dKlk6/PIgHZxh7j9Fq4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Connect to Google Drive via Google Colab"],"metadata":{"id":"dIf-siSTXIHR"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6R907Jerc49k","executionInfo":{"status":"ok","timestamp":1658335126714,"user_tz":420,"elapsed":2772,"user":{"displayName":"Leon Shpaner","userId":"06547645094449658414"}},"outputId":"391b29e5-88db-4726-9d37-fa3abdcf1ab1"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","os.getcwd()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"cRLxjEAUMK3Q","executionInfo":{"status":"ok","timestamp":1658335126715,"user_tz":420,"elapsed":12,"user":{"displayName":"Leon Shpaner","userId":"06547645094449658414"}},"outputId":"387b7cac-727d-4db9-b43a-a5b10845fb1c"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["%cd '/content/drive/My Drive/Python Projects/san_diego_street_conditions'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p09AdCgwPJfx","executionInfo":{"status":"ok","timestamp":1658335126716,"user_tz":420,"elapsed":10,"user":{"displayName":"Leon Shpaner","userId":"06547645094449658414"}},"outputId":"fc560a05-db16-485e-e1d6-1b584b92d329"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/Python Projects/san_diego_street_conditions\n"]}]},{"cell_type":"markdown","source":["## San Diego Street Conditions Classification Report - Full Code"],"metadata":{"id":"OIvt7wWRW-fE"}},{"cell_type":"code","source":["# import the requisite libraries\n","import pandas as pd\n","import csv\n","from tabulate import tabulate\n","\n","# read in the dataframes from GitHub\n","df1 = pd.read_csv('https://raw.githubusercontent.com/lshpaner/sd_streets/' \\\n","                  + 'main/data/oci_2015_datasd.csv')\n","df2 = pd.read_csv('https://raw.githubusercontent.com/lshpaner/sd_streets/' \\\n","                  + 'main/data/sd_paving_datasd.csv')\n","df3 = pd.read_csv('https://raw.githubusercontent.com/lshpaner/sd_streets/' \\\n","                  + 'main/data/traffic_counts_datasd.csv')\n","df4 = pd.read_csv('https://raw.githubusercontent.com/lshpaner/sd_streets/' \\\n","                  + 'main/merged_data/sd_roads_dataframe.csv')\n","path = '/content/drive/My Drive/Python Projects/san_diego_street_conditions'\n","\n","def build_report(df1, df2, df3, df4):\n","  # create an empty log container for appending to output dataframe \n","  log_txt = []\n","\n","  # file names\n","  names = ['Streets OCI', 'Street Repair Projects', \n","          'Traffic Volumes', 'Merged Dataframe']\n","  dataframes = df1, df2, df3, df4 # dataframes for files\n","\n","  print(' ')\n","  # append header to log_txt container defined above\n","  log_txt.append(' ')\n","  \n","  # shape of merged dataframes file\n","  print('Merged File')\n","  log_txt.append('Merged File')\n","  print('No. of Rows in Merged File: ' + str(f\"{df4.shape[0]:,}\"))\n","  log_txt.append('No. of Rows in Merged File: ' + str(f\"{df4.shape[0]:,}\"))\n","  print('No. of Columns in Merged File: ' + str(f\"{df4.shape[1]:,}\"))\n","  log_txt.append('No. of Columns in Merged File: ' + str(f\"{df4.shape[1]:,}\"))\n","  print(' ')\n","  log_txt.append(' ')\n","\n","  # OCI file\n","  print('Streets Overall Condition Index (OCI)')\n","  log_txt.append('Streets Overall Condition Index (OCI)')\n","  print('No. of Rows in OCI File: ' + str(f\"{df1.shape[0]:,}\"))\n","  log_txt.append('No. of Rows in OCI File: ' + str(f\"{df1.shape[0]:,}\"))\n","  print('No. of Columns in OCI File: ' + str(f\"{df1.shape[1]:,}\"))\n","  log_txt.append('No. of Columns in OCI File: ' + str(f\"{df1.shape[1]:,}\"))\n","\n","  print(' ')\n","  log_txt.append(' ')\n","\n","  # filter out any columns contain the '_id' string\n","  oci_id = df1.filter(like='_id').columns\n","  # if there are any columns that contain the '_id' string in OCI\n","  # print the number of unique columns and get a distinct count\n","  # otherwise, report that these OCI Ids do not exist.\n","  if df1[oci_id].columns.any():\n","    df1_print = df1[oci_id].nunique().apply(lambda x : \"{:,}\".format(x))\n","    df1_print = pd.DataFrame(df1_print)\n","    df1_print.reset_index(inplace=True)\n","    df1_print = df1_print.rename(columns={0: 'Distinct Count',\n","                                            'index':'ID Columns'})\n","    # encapsulate this distinct count within a table\n","    df1_tab = tabulate(df1_print, headers='keys', tablefmt='psql')\n","    print(df1_tab)\n","    log_txt.append(df1_tab)\n","  else:\n","    df1_notab = 'Street OCI IDs DO NOT exist.'\n","    print(df1_notab)\n","    log_txt.append(df1_notab)\n","\n","  print(' ')\n","  log_txt.append(' ')\n","\n","  # Street Repair Projects File\n","  print('Street Repair Projects')\n","  log_txt.append('Street Repair Projects')\n","  print('No. of Rows in Street Repairs File: ' + \\\n","        str(f\"{df2.shape[0]:,}\"))\n","  log_txt.append('No. of Rows in Street Repairs File: ' + \\\n","                 str(f\"{df2.shape[0]:,}\"))\n","  print('No. of Columns in Street Repairs File: ' + \\\n","        str(f\"{df2.shape[1]:,}\"))\n","  log_txt.append('No. of Columns in Street Repairs File: ' + \\\n","                 str(f\"{df2.shape[1]:,}\"))\n","\n","  print(' ')\n","  log_txt.append(' ')\n","\n","  streets_id = df2.filter(like='_id').columns\n","  if df2[oci_id].columns.any():\n","    df2_print = df2[streets_id].nunique().apply(lambda x : \"{:,}\".format(x))\n","    df2_print = pd.DataFrame(df2_print)\n","    df2_print.reset_index(inplace=True)\n","    df2_print = df2_print.rename(columns={0:'Distinct Count',\n","                                            'index':'ID Columns'})\n","    df2_tab = tabulate(df2_print, headers='keys',tablefmt='psql')\n","    print(df2_tab)\n","    log_txt.append(df2_tab)\n","  else:\n","    df2_notab= 'Street Repairs IDs DO NOT exist.'\n","    print(df2_notab)\n","    log_txt.append(df2_notab)\n","\n","  print(' ')\n","  log_txt.append(' ')\n","\n","  # shape of Traffic Volumes File\n","  print('Traffic Volumes')\n","  log_txt.append('Traffic Volumes')\n","  print('No. of Rows in Traffic Volumes File: ' + \\\n","        str(f\"{df3.shape[0]:,}\"))\n","  log_txt.append('No. of Rows in Traffic Volumes File: ' + \\\n","                 str(f\"{df3.shape[0]:,}\"))\n","  print('No. of Columns in Traffic Volumes File: ' + \\\n","        str(f\"{df3.shape[1]:,}\"))\n","  log_txt.append('No. of Columns in Traffic Volumes File: ' + \\\n","                 str(f\"{df3.shape[1]:,}\"))\n","\n","  print(' ')\n","  log_txt.append(' ')\n","\n","  traffic_id = df3.filter(like='_id').columns\n","  if df3[traffic_id].columns.any():\n","    df3_print = df3[traffic_id].nunique().apply(lambda x : \"{:,}\".format(x))\n","    df3_print = pd.DataFrame(df3_print)\n","    df3_print.reset_index(inplace=True)\n","    df3_print = df3_print.rename(columns={0:'Distinct Count',\n","                                          'index':'ID Columns'})\n","    df3_tab = tabulate(df3_print, headers='keys', tablefmt='psql')\n","    print(df3_tab)\n","    log_txt.append(df3_tab)\n","  else:\n","    df3_notab = 'Traffic Volume IDs DO NOT exist.'\n","    print(df3_notab)\n","    log_txt.append(df3_notab)\n","\n","  print(' ')\n","  log_txt.append(' ')\n","\n","  ###########################\n","  ### Cross-file matching ###\n","  ###########################\n","\n","  df1_df2 = set(df1.columns).intersection(set(df2.columns))\n","  df1_df2 = list(df1_df2)\n","  if len(df1_df2) != 0:\n","    df1_df2 = pd.DataFrame(df1_df2)\n","    df1_df2 = df1_df2.rename(columns={0:'Shared Columns Between Street Repairs'\n","                                      +' and OCI File'})\n","    df1_df2_tab = (tabulate(df1_df2, headers='keys', tablefmt='psql'))\n","    print(df1_df2_tab)\n","    log_txt.append(df1_df2_tab)\n","  else:\n","    df1_df2_notab = 'There are no shared columns between Street Repairs file' + \\\n","                    ' and OCI file.'\n","    print(df1_df2_notab)                \n","    log_txt.append(df1_df2_notab)\n","\n","  print(' ')\n","  log_txt.append(' ')\n","\n","  # matching `pve_id` in dataframes\n","  print(tabulate({'Matching Pve_IDs Across Files':\\\n","                       ['Matching Pve_IDs Across Files']}, headers='keys'))\n","  log_txt.append(tabulate({'Matching Pve_IDs Across Files':\\\n","                       ['Matching Pve_IDs Across Files']}, headers='keys'))\n","  for name, dataframe in zip(names, dataframes):\n","    if 'pve_id' in dataframe:\n","      pve_id = name+': pve_id' + ' = ' + str(True)\n","      print(pve_id)\n","      log_txt.append(pve_id)\n","    else:\n","      no_pve_id = name + ': pve_id' + ' = ' + str(False)\n","      print(no_pve_id)\n","      log_txt.append(no_pve_id)\n","\n","  print(' ')\n","  log_txt.append(' ')\n","\n","  for name, dataframe in zip(names, dataframes):\n","    if 'pve_id' in dataframe:\n","      pve_id_exists = 'There are ' + str(f\"\"\"{(dataframe['pve_id'].isin(dataframe\n","                                          ['pve_id']).astype(int).value_counts()\n","                                          [1]):,}\"\"\") + ' ' + name + \\\n","                                           ' pve_ids in Streets OCI.'\n","      print(pve_id_exists)                \n","      log_txt.append(pve_id_exists)\n","    else: \n","      no_pve_id = 'There are no ' + name + ' pve_ids ' + 'in Streets OCI.'\n","      print(no_pve_id)\n","      log_txt.append(no_pve_id)\n","  \n","  print(' ')\n","  log_txt.append(' ')\n","\n","\n","  # matching `seg_id` in dataframes\n","  print(tabulate({'Matching Seg_IDs Across Files':\\\n","                 ['Matching Seg_IDs Across Files']}, headers='keys'))\n","  \n","  log_txt.append(tabulate({'Matching Seg_IDs Across Files':\\\n","                       ['Matching Seg_IDs Across Files']}, headers='keys'))\n","\n","  for name, dataframe in zip(names, dataframes):\n","  # check for `seg_ids` in dataframes\n","    if 'seg_id' in dataframe:\n","      seg_log = name + ': seg_id' + ' = ' + str(True)\n","      print(seg_log)\n","      log_txt.append(seg_log)\n","      seg_summary = 'There are ' + str(f\"{dataframe['seg_id'].nunique():,}\") \\\n","      + ' unique seg_id values in ' + name + '.'\n","      print(seg_summary)\n","      log_txt.append(seg_summary)\n","    else:\n","      no_seg_log = name + ': seg_id' + ' = ' + str(False)\n","      print(no_seg_log)\n","      log_txt.append(no_seg_log)\n","\n","  print(' ')\n","  log_txt.append(' ')\n","\n","  for name, dataframe in zip(names, dataframes):\n","    if 'seg_id' in dataframe:\n","      seg_id_exists = 'There are '+str(f\"\"\"{(dataframe['seg_id'].isin(dataframe\n","                                          ['seg_id']).astype(int).value_counts()\n","                                          [1]):,}\"\"\") + ' ' + name + \\\n","                      ' seg_ids in Street OCI.'\n","      print(seg_id_exists)\n","      log_txt.append(seg_id_exists)\n","    else: \n","      no_seg_id = 'There are no ' + name +' seg_ids ' + \\\n","      'in Streets OCI.'\n","      print(no_seg_id)\n","      log_txt.append(no_seg_id)\n","\n","  report = pd.DataFrame({'San Diego Street Conditions Classification Report'\n","                          :log_txt})\n","  \n","  return report\n","\n","# pass the build_report function to a new variable named report\n","report = build_report(df1, df2, df3, df4)\n","\n","## EXTRA FUNCTION DEFINES WHAT GETS SENT OUT\n","\n","# save report to .txt file\n","report.to_csv(path + '/report.txt', index=False, sep=\"\\t\",\n","              quoting=csv.QUOTE_NONE, quotechar='', escapechar='\\t')\n","\n","# save report to .rtf file\n","report.to_csv(path + '/report.rtf', index=False, sep=\"\\t\",\n","              quoting=csv.QUOTE_NONE, quotechar='', escapechar='\\t')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LcyAZ57iLN0Z","executionInfo":{"status":"ok","timestamp":1658335129594,"user_tz":420,"elapsed":2885,"user":{"displayName":"Leon Shpaner","userId":"06547645094449658414"}},"outputId":"7df35959-58ec-48c8-fd2c-39aa785ac5a1"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":[" \n","Merged File\n","No. of Rows in Merged File: 22,998\n","No. of Columns in Merged File: 15\n"," \n","Streets Overall Condition Index (OCI)\n","No. of Rows in OCI File: 30,712\n","No. of Columns in OCI File: 12\n"," \n","+----+--------------+------------------+\n","|    | ID Columns   | Distinct Count   |\n","|----+--------------+------------------|\n","|  0 | seg_id       | 30,712           |\n","+----+--------------+------------------+\n"," \n","Street Repair Projects\n","No. of Rows in Street Repairs File: 23,433\n","No. of Columns in Street Repairs File: 19\n"," \n","+----+--------------+------------------+\n","|    | ID Columns   | Distinct Count   |\n","|----+--------------+------------------|\n","|  0 | pve_id       | 23,433           |\n","|  1 | seg_id       | 18,072           |\n","|  2 | project_id   | 103              |\n","+----+--------------+------------------+\n"," \n","Traffic Volumes\n","No. of Rows in Traffic Volumes File: 12,390\n","No. of Columns in Traffic Volumes File: 10\n"," \n","Traffic Volume IDs DO NOT exist.\n"," \n","+----+------------------------------------------------------+\n","|    | Shared Columns Between Street Repairs and OCI File   |\n","|----+------------------------------------------------------|\n","|  0 | seg_id                                               |\n","|  1 | street_to                                            |\n","|  2 | street_from                                          |\n","+----+------------------------------------------------------+\n"," \n","Matching Pve_IDs Across Files\n","-------------------------------\n","Matching Pve_IDs Across Files\n","Streets OCI: pve_id = False\n","Street Repair Projects: pve_id = True\n","Traffic Volumes: pve_id = False\n","Merged Dataframe: pve_id = False\n"," \n","There are no Streets OCI pve_ids in Streets OCI.\n","There are 23,433 Street Repair Projects pve_ids in Streets OCI.\n","There are no Traffic Volumes pve_ids in Streets OCI.\n","There are no Merged Dataframe pve_ids in Streets OCI.\n"," \n","Matching Seg_IDs Across Files\n","-------------------------------\n","Matching Seg_IDs Across Files\n","Streets OCI: seg_id = True\n","There are 30,712 unique seg_id values in Streets OCI.\n","Street Repair Projects: seg_id = True\n","There are 18,072 unique seg_id values in Street Repair Projects.\n","Traffic Volumes: seg_id = False\n","Merged Dataframe: seg_id = False\n"," \n","There are 30,712 Streets OCI seg_ids in Street OCI.\n","There are 23,433 Street Repair Projects seg_ids in Street OCI.\n","There are no Traffic Volumes seg_ids in Streets OCI.\n","There are no Merged Dataframe seg_ids in Streets OCI.\n"]}]},{"cell_type":"code","source":["!jupyter nbconvert --to html sd_streets_ids.ipynb"],"metadata":{"id":"Qo4g9FJHi1OY","executionInfo":{"status":"ok","timestamp":1658335131890,"user_tz":420,"elapsed":2304,"user":{"displayName":"Leon Shpaner","userId":"06547645094449658414"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"de278df9-bf69-4a96-8723-04fa1bd527df"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[NbConvertApp] Converting notebook sd_streets_ids.ipynb to html\n","[NbConvertApp] Writing 324992 bytes to sd_streets_ids.html\n"]}]}]}